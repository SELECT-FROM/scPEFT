{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a4fbd7",
   "metadata": {},
   "source": [
    "# PEFT(Parameter-Efficient Fine-Tuning) on Pre-trained Model for Cross Species\n",
    "In this tutorial, we demonstrate how to peft(Parameter-Efficient Fine-Tuning) a pre-trained model on a new dataset for the cross species task. We use the Multiple Sclerosis dataset as an example and peft on the pre-trained whole-body model. \n",
    "\n",
    "We summarize the peft pipeline in the following steps, which can be used as a general recipe for parameter-efficient finetuning on cross species tasks and beyond: \n",
    "\n",
    "     1. Specify hyper-parameter setup for species task\n",
    "     \n",
    "     2. Load the processed data by Tutorial_HomologeneMapping.ipynb and pre-process data\n",
    "     \n",
    "     3. Load the pre-trained scGPT model\n",
    "     \n",
    "     4. PEFT scGPT with task-specific objectives\n",
    "     \n",
    "     5. Evaluate PEFT scGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9406b4da",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCON0022/coffee19850519/ondemand/scPEFT-main/tutorial_peft/../scgpt/model/model.py:22: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/users/PCON0022/coffee19850519/ondemand/scPEFT-main/tutorial_peft/../scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, PeftConfig, freeze_parameters, DownstreamTasks, load_pretrained\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b5d67",
   "metadata": {},
   "source": [
    "## Step1: Specify hyper-parameter setup for cross species task\n",
    "Listed below are some hyper-parameter recommendations for the cross species task. Note that the CLS objective is on to facilitate cross species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07b5257",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    dataset_name=\"Elegans\",\n",
    "    do_train=True,\n",
    "    load_model=\"../save/scGPT_human\",\n",
    "    mask_ratio=0.0,\n",
    "    epochs=20,\n",
    "    n_bins=51,\n",
    "    MVC=False,  # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.0,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=0.0,\n",
    "    lr=5e-5,\n",
    "    batch_size=50,\n",
    "    dropout=0.2,  # dropout probability\n",
    "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5,\n",
    "    fast_transformer=False,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    include_zero_gene=False,\n",
    "    freeze=False,  # freeze\n",
    "    DSBN=False,  # Domain-spec batchnorm\n",
    "    peft=\"ENCODER\"\n",
    "    # Whether using Parameter-Efficient Fine-Tuning, \n",
    "    # False to disable, HYBRID/ENCODER/TOKEN/PREFIX/LORA are available for selection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94c08ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(seed=0, dataset_name='Elegans', do_train=True, load_model='../save/scGPT_human', mask_ratio=0.0, epochs=20, n_bins=51, MVC=False, ecs_thres=0.0, dab_weight=0.0, lr=5e-05, batch_size=50, dropout=0.2, schedule_ratio=0.9, save_eval_interval=5, fast_transformer=False, pre_norm=False, amp=True, include_zero_gene=False, freeze=False, DSBN=False, peft='ENCODER')\n"
     ]
    }
   ],
   "source": [
    "config = argparse.Namespace(**hyperparameter_defaults)\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d7890b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "include_zero_gene = config.include_zero_gene  # if True, include zero genes among hvgs in the training\n",
    "max_seq_len = 2001\n",
    "n_bins = config.n_bins\n",
    "\n",
    "# settings for parameter efficient fine tuning\n",
    "assert config.peft in [False, \"HYBRID\", \"ENCODER\", \"TOKEN\", \"PREFIX\", \"LORA\"]\n",
    "peft_config = PeftConfig(peft_type=config.peft, use_default_settings=True).to_dict()\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "output_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "\n",
    "# settings for training\n",
    "MLM = False  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = True  # celltype classification objective\n",
    "ADV = False  # Adversarial training for batch correction\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = config.MVC  # Masked value prediction for cell embedding\n",
    "ECS = config.ecs_thres > 0  # Elastic cell similarity objective\n",
    "DAB = False  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer\n",
    "INPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\n",
    "input_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "adv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\n",
    "adv_D_delay_epochs = 0\n",
    "mvc_decoder_style = \"inner product\"\n",
    "ecs_threshold = config.ecs_thres\n",
    "dab_weight = config.dab_weight\n",
    "\n",
    "explicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\n",
    "do_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n",
    "\n",
    "per_seq_batch_sample = False\n",
    "\n",
    "# settings for optimizer\n",
    "lr = config.lr  # TODO: test learning rate ratio between two tasks\n",
    "lr_ADV = 1e-3  # learning rate for discriminator, used when ADV is True\n",
    "early_stop = 15\n",
    "batch_size = config.batch_size\n",
    "eval_batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "schedule_interval = 1\n",
    "\n",
    "# settings for the model\n",
    "fast_transformer = config.fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "dropout = config.dropout  # dropout probability\n",
    "\n",
    "# logging\n",
    "log_interval = 100  # iterations\n",
    "save_eval_interval = config.save_eval_interval  # epochs\n",
    "do_eval_scib_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ff2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% validate settings\n",
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "if ADV and DAB:\n",
    "    raise ValueError(\"ADV and DAB cannot be both True.\")\n",
    "DAB_separate_optim = True if DAB > 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7112d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save/peft_Elegans-Apr02-12-52\n"
     ]
    }
   ],
   "source": [
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"./save/peft_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc7002",
   "metadata": {},
   "source": [
    "## Step 2: Load the processed data by Tutorial_HomologeneMapping.ipynb and pre-process data\n",
    "Firstly, you should use Tutorial_HomologeneMapping.ipynb to get homologene from non-human to human. And then We follow the standard scGPT data pre-processing pipelines for the cross species task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b50200",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"Elegans\":\n",
    "    data_dir = Path(\"../data/celltype_annotation/Elegans\")\n",
    "    \n",
    "    adata = sc.read(data_dir / \"Elegans_train.h5ad\")\n",
    "    adata_val = sc.read(data_dir / \"Elegans_valid.h5ad\")\n",
    "    adata_test = sc.read(data_dir / \"Elegans_test.h5ad\")\n",
    "\n",
    "    adata.obs[\"batch_id\"] = adata.obs[\"str_batch\"] = \"0\"\n",
    "    adata_val.obs[\"batch_id\"] = adata_val.obs[\"str_batch\"] = \"1\"\n",
    "    adata_test.obs[\"batch_id\"] = adata_test.obs[\"str_batch\"] = \"2\"\n",
    "    \n",
    "    adata.var[\"gene_name\"] =adata.var[\"homolog\"]\n",
    "    adata_val.var[\"gene_name\"] = adata_val.var[\"homolog\"]\n",
    "    adata_test.var[\"gene_name\"] =adata_test.var[\"homolog\"]\n",
    "\n",
    "    data_is_raw = True\n",
    "    filter_gene_by_counts = False\n",
    "    adata = adata.concatenate((adata_test,adata_val), batch_key=\"str_batch\")\n",
    "\n",
    "# make the batch category column\n",
    "batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "celltype_id_labels = adata.obs[\"tissue_name\"].astype(\"category\").cat.codes.values\n",
    "celltypes = adata.obs[\"tissue_name\"].unique()\n",
    "num_types = len(np.unique(celltype_id_labels))\n",
    "id2type = dict(enumerate(adata.obs[\"tissue_name\"].astype(\"category\").cat.categories))\n",
    "adata.obs[\"celltype_id\"] = celltype_id_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dc5a6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 1866/2000 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from ../save/scGPT_human/best_model.pt, the model args will override the config ../save/scGPT_human/args.json.\n"
     ]
    }
   ],
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d08757ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Log1p transforming ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Log1p transforming ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - WARNING - The input data contains all zero rows. Please make sure this is expected. You can use the `filter_cell_by_counts` arg to filter out all zero rows.\n"
     ]
    }
   ],
   "source": [
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "adata_test = adata[adata.obs[\"str_batch\"] == \"2\"]\n",
    "adata = adata[adata.obs[\"str_batch\"] != \"2\"]\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n",
    "preprocessor(adata_test, batch_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbc1b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "train_celltype_labels = adata[adata.obs[\"str_batch\"] == \"0\"].obs[\"celltype_id\"].values  # make sure count from 0\n",
    "valid_celltype_labels = adata[adata.obs[\"str_batch\"] == \"1\"].obs[\"celltype_id\"].values  # make sure count from 0\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "\n",
    "train_batch_labels = adata[adata.obs[\"str_batch\"] == \"0\"].obs[\"batch_id\"].values\n",
    "valid_batch_labels = adata[adata.obs[\"str_batch\"] == \"1\"].obs[\"batch_id\"].values\n",
    "\n",
    "adata_val = adata[adata.obs[\"str_batch\"] == \"1\"]\n",
    "adata = adata[adata.obs[\"str_batch\"] == \"0\"]\n",
    "\n",
    "train_data = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "\n",
    "valid_data = (\n",
    "    adata_val.layers[input_layer_key].A\n",
    "    if issparse(adata_val.layers[input_layer_key])\n",
    "    else adata_val.layers[input_layer_key]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cd701b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "818bfcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 41748, \n",
      "\t feature length: 736\n",
      "scGPT - INFO - valid set number of samples: 4484, \n",
      "\t feature length: 555\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37a80818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    tensor_celltype_labels_train = torch.from_numpy(train_celltype_labels).long()\n",
    "    tensor_celltype_labels_valid = torch.from_numpy(valid_celltype_labels).long()\n",
    "\n",
    "    if sort_seq_batch:  # TODO: update to random pick seq source in each traning batch\n",
    "        train_sort_ids = np.argsort(train_batch_labels)\n",
    "        input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "        input_values_train = input_values_train[train_sort_ids]\n",
    "        target_values_train = target_values_train[train_sort_ids]\n",
    "        tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "        tensor_celltype_labels_train = tensor_celltype_labels_train[train_sort_ids]\n",
    "\n",
    "        valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "        input_values_valid = input_values_valid[valid_sort_ids]\n",
    "        target_values_valid = target_values_valid[valid_sort_ids]\n",
    "        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "        tensor_celltype_labels_valid = tensor_celltype_labels_valid[valid_sort_ids]\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "        \"batch_labels\": tensor_batch_labels_train,\n",
    "        \"celltype_labels\": tensor_celltype_labels_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "        \"batch_labels\": tensor_batch_labels_valid,\n",
    "        \"celltype_labels\": tensor_celltype_labels_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "        data_pt: Dict[str, torch.Tensor],\n",
    "        batch_size: int,\n",
    "        shuffle: bool = False,\n",
    "        intra_domain_shuffle: bool = False,\n",
    "        drop_last: bool = False,\n",
    "        num_workers: int = 0,\n",
    "        sampler: torch.utils.data.Sampler = None,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77105fda",
   "metadata": {},
   "source": [
    "## Step 3: Load the pre-trained scGPT model\n",
    "We load the pre-trained scGPT model and freeze specific parameters. The specific parameters to be frozen or opened depend on the downstream tasks and the type of adapter used. Here, we only need to specify specific downstream tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "219bb9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.0.space_adapter.fc1.weight with shape torch.Size([128, 512])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.0.space_adapter.fc1.bias with shape torch.Size([128])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.0.space_adapter.fc2.weight with shape torch.Size([512, 128])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.0.space_adapter.fc2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.1.space_adapter.fc1.weight with shape torch.Size([128, 512])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.1.space_adapter.fc1.bias with shape torch.Size([128])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.1.space_adapter.fc2.weight with shape torch.Size([512, 128])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.1.space_adapter.fc2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.2.space_adapter.fc1.weight with shape torch.Size([128, 512])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.2.space_adapter.fc1.bias with shape torch.Size([128])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.2.space_adapter.fc2.weight with shape torch.Size([512, 128])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.2.space_adapter.fc2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.3.space_adapter.fc1.weight with shape torch.Size([128, 512])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.3.space_adapter.fc1.bias with shape torch.Size([128])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.3.space_adapter.fc2.weight with shape torch.Size([512, 128])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.3.space_adapter.fc2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.4.space_adapter.fc1.weight with shape torch.Size([128, 512])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.4.space_adapter.fc1.bias with shape torch.Size([128])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.4.space_adapter.fc2.weight with shape torch.Size([512, 128])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.4.space_adapter.fc2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.5.space_adapter.fc1.weight with shape torch.Size([128, 512])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.5.space_adapter.fc1.bias with shape torch.Size([128])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.5.space_adapter.fc2.weight with shape torch.Size([512, 128])\n",
      "scGPT - INFO - Learnable params transformer_encoder.layers.5.space_adapter.fc2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Learnable params cls_decoder._decoder.0.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Learnable params cls_decoder._decoder.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Learnable params cls_decoder._decoder.2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Learnable params cls_decoder._decoder.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Learnable params cls_decoder._decoder.3.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Learnable params cls_decoder._decoder.3.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Learnable params cls_decoder._decoder.5.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Learnable params cls_decoder._decoder.5.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Learnable params cls_decoder.out_layer.weight with shape torch.Size([18, 512])\n",
      "scGPT - INFO - Learnable params cls_decoder.out_layer.bias with shape torch.Size([18])\n",
      "scGPT - INFO - Total Pre freeze Params: 52.13M\n",
      "scGPT - INFO - Total Post freeze Params: 1.33M\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=num_types if CLS else 1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=MVC,\n",
    "    do_dab=DAB,\n",
    "    use_batch_labels=INPUT_BATCH_LABELS,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=config.DSBN,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    mvc_decoder_style=mvc_decoder_style,\n",
    "    ecs_threshold=ecs_threshold,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=config.pre_norm,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    load_pretrained(model, torch.load(model_file), verbose=False)\n",
    "\n",
    "pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "# Freeze params\n",
    "if config.peft:\n",
    "    freeze_parameters(model, DownstreamTasks.Identification)\n",
    "\n",
    "post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "logger.info(\"-\" * 89)\n",
    "learnable_params = {k: v for k, v in model.named_parameters() if v.requires_grad}\n",
    "for k, v in learnable_params.items():\n",
    "    logger.info(f\"Learnable params {k} with shape {v.shape}\")\n",
    "\n",
    "logger.info(\"Total Pre freeze Params: %.2fM\" % (pre_freeze_param_count / 1e6,))\n",
    "logger.info(\"Total Post freeze Params: %.2fM\" % (post_freeze_param_count / 1e6,))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "if ADV:\n",
    "    discriminator = AdversarialDiscriminator(\n",
    "        d_model=embsize,\n",
    "        n_cls=num_batch_types,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e4ea79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a weight to each type of cell based on the proportion of cell types to facilitate better attention to fewer cell types during training\n",
    "class_num = np.unique(celltype_id_labels, return_counts=True)[1].tolist()\n",
    "class_weight = torch.tensor([(1 - (x / sum(class_num))) ** 2 for x in class_num]).to(device)\n",
    "\n",
    "criterion = masked_mse_loss\n",
    "criterion_cls = nn.CrossEntropyLoss(weight=class_weight)\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, schedule_interval, gamma=config.schedule_ratio\n",
    ")\n",
    "if DAB_separate_optim:\n",
    "    optimizer_dab = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler_dab = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_dab, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "if ADV:\n",
    "    criterion_adv = nn.CrossEntropyLoss()  # consider using label smoothing\n",
    "    optimizer_E = torch.optim.Adam(model.parameters(), lr=lr_ADV)\n",
    "    scheduler_E = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_E, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_ADV)\n",
    "    scheduler_D = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_D, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b734269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    (\n",
    "        total_loss,\n",
    "        total_mse,\n",
    "        total_cls,\n",
    "        total_cce,\n",
    "        total_mvc,\n",
    "        total_ecs,\n",
    "        total_dab,\n",
    "        total_adv_E,\n",
    "        total_adv_D,\n",
    "        total_zero_log_prob,\n",
    "        total_mvc_zero_log_prob,\n",
    "    ) = (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "    total_error = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "        batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "        celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "                do_sample=do_sample_in_train,\n",
    "                #generative_training=False\n",
    "            )\n",
    "\n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = 0.0\n",
    "            metrics_to_log = {}\n",
    "            if MLM:\n",
    "                loss_mse = criterion(\n",
    "                    output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mse\n",
    "                metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            if explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mlm_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "            if CLS:\n",
    "                loss_cls = criterion_cls(output_dict[\"cls_output\"], celltype_labels)\n",
    "                loss = loss + loss_cls\n",
    "                metrics_to_log.update({\"train/cls\": loss_cls.item()})\n",
    "\n",
    "                error_rate = 1 - (\n",
    "                    (output_dict[\"cls_output\"].argmax(1) == celltype_labels)\n",
    "                    .sum()\n",
    "                    .item()\n",
    "                ) / celltype_labels.size(0)\n",
    "            if CCE:\n",
    "                loss_cce = 10 * output_dict[\"loss_cce\"]\n",
    "                loss = loss + loss_cce\n",
    "                metrics_to_log.update({\"train/cce\": loss_cce.item()})\n",
    "            if MVC:\n",
    "                loss_mvc = criterion(\n",
    "                    output_dict[\"mvc_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mvc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_mvc.item()})\n",
    "            if MVC and explicit_zero_prob:\n",
    "                loss_mvc_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mvc_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mvc_zero_log_prob\n",
    "                metrics_to_log.update({\"train/mvc_nzlp\": loss_mvc_zero_log_prob.item()})\n",
    "            if ECS:\n",
    "                loss_ecs = 10 * output_dict[\"loss_ecs\"]\n",
    "                loss = loss + loss_ecs\n",
    "                metrics_to_log.update({\"train/ecs\": loss_ecs.item()})\n",
    "            if DAB:\n",
    "                # try weighting and separate optimizer\n",
    "                loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "                loss = loss + dab_weight * loss_dab\n",
    "                metrics_to_log.update({\"train/dab\": loss_dab.item()})\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if ADV:\n",
    "            # rerun the model for adversarial training\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "                do_sample=do_sample_in_train,\n",
    "                #generative_training=False\n",
    "            )\n",
    "\n",
    "            # TRAINING DISCRIMINATOR\n",
    "            loss_adv_D = criterion_adv(\n",
    "                discriminator(output_dict[\"cell_emb\"].detach()), batch_labels\n",
    "            )\n",
    "            if epoch > adv_D_delay_epochs:\n",
    "                discriminator.zero_grad()\n",
    "                loss_adv_D.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "            # TRAINING ENCODER\n",
    "            loss_adv_E = -criterion_adv(\n",
    "                discriminator(output_dict[\"cell_emb\"]), batch_labels\n",
    "            )\n",
    "            # NOTE: the loss is negative here because we want to maximize\n",
    "            # the cross_entropy_loss, in other words, disguise against the discriminator\n",
    "            if epoch > adv_E_delay_epochs:\n",
    "                model.zero_grad()\n",
    "                discriminator.zero_grad()\n",
    "                loss_adv_E.backward()\n",
    "                optimizer_E.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_mse.item() if MLM else 0.0\n",
    "        total_cls += loss_cls.item() if CLS else 0.0\n",
    "        total_cce += loss_cce.item() if CCE else 0.0\n",
    "        total_mvc += loss_mvc.item() if MVC else 0.0\n",
    "        total_ecs += loss_ecs.item() if ECS else 0.0\n",
    "        total_dab += loss_dab.item() if DAB else 0.0\n",
    "        total_adv_E += loss_adv_E.item() if ADV else 0.0\n",
    "        total_adv_D += loss_adv_D.item() if ADV else 0.0\n",
    "        total_zero_log_prob += loss_zero_log_prob.item() if explicit_zero_prob else 0.0\n",
    "        total_mvc_zero_log_prob += (\n",
    "            loss_mvc_zero_log_prob.item() if MVC and explicit_zero_prob else 0.0\n",
    "        )\n",
    "        total_error += error_rate\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_cls = total_cls / log_interval if CLS else 0.0\n",
    "            cur_cce = total_cce / log_interval if CCE else 0.0\n",
    "            cur_mvc = total_mvc / log_interval if MVC else 0.0\n",
    "            cur_ecs = total_ecs / log_interval if ECS else 0.0\n",
    "            cur_dab = total_dab / log_interval if DAB else 0.0\n",
    "            cur_adv_E = total_adv_E / log_interval if ADV else 0.0\n",
    "            cur_adv_D = total_adv_D / log_interval if ADV else 0.0\n",
    "            cur_zero_log_prob = (\n",
    "                total_zero_log_prob / log_interval if explicit_zero_prob else 0.0\n",
    "            )\n",
    "            cur_mvc_zero_log_prob = (\n",
    "                total_mvc_zero_log_prob / log_interval\n",
    "                if MVC and explicit_zero_prob\n",
    "                else 0.0\n",
    "            )\n",
    "            cur_error = total_error / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.5f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | \"\n",
    "                + (f\"mse {cur_mse:5.2f} | mre {cur_error:5.2f} |\" if MLM else \"\")\n",
    "                + (f\"cls {cur_cls:5.2f} | \" if CLS else \"\")\n",
    "                + (f\"err {cur_error:5.2f} | \" if CLS else \"\")\n",
    "                + (f\"cce {cur_cce:5.2f} |\" if CCE else \"\")\n",
    "                + (f\"mvc {cur_mvc:5.2f} |\" if MVC else \"\")\n",
    "                + (f\"ecs {cur_ecs:5.2f} |\" if ECS else \"\")\n",
    "                + (f\"dab {cur_dab:5.2f} |\" if DAB else \"\")\n",
    "                + (f\"adv_E {cur_adv_E:5.2f} |\" if ADV else \"\")\n",
    "                + (f\"adv_D {cur_adv_D:5.2f} |\" if ADV else \"\")\n",
    "                + (f\"nzlp {cur_zero_log_prob:5.2f} |\" if explicit_zero_prob else \"\")\n",
    "                + (\n",
    "                    f\"mvc_nzlp {cur_mvc_zero_log_prob:5.2f} |\"\n",
    "                    if MVC and explicit_zero_prob\n",
    "                    else \"\"\n",
    "                )\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_cls = 0\n",
    "            total_cce = 0\n",
    "            total_mvc = 0\n",
    "            total_ecs = 0\n",
    "            total_dab = 0\n",
    "            total_adv_E = 0\n",
    "            total_adv_D = 0\n",
    "            total_zero_log_prob = 0\n",
    "            total_mvc_zero_log_prob = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, return_raw: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "            celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                    CLS=CLS,  # evaluation does not need CLS or CCE\n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=do_sample_in_train,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "                output_values = output_dict[\"cls_output\"]\n",
    "                loss = criterion_cls(output_values, celltype_labels)\n",
    "\n",
    "                if DAB:\n",
    "                    loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            accuracy = (output_values.argmax(1) == celltype_labels).sum().item()\n",
    "            total_error += (1 - accuracy / len(input_gene_ids)) * len(input_gene_ids)\n",
    "            total_dab += loss_dab.item() * len(input_gene_ids) if DAB else 0.0\n",
    "            total_num += len(input_gene_ids)\n",
    "            preds = output_values.argmax(1).cpu().numpy()\n",
    "            predictions.append(preds)\n",
    "\n",
    "    if return_raw:\n",
    "        return np.concatenate(predictions, axis=0)\n",
    "\n",
    "    return total_loss / total_num, total_error / total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6442de836823991",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "\n",
    "# Add a weighted sampler to the dataloader based on the number of cells in the training set\n",
    "class_counts = np.unique(train_data_pt['celltype_labels'], return_counts=True)[1]\n",
    "class_weights = 1.0 / class_counts[train_data_pt['celltype_labels']]\n",
    "sample_weights = class_weights / np.sum(class_weights)\n",
    "train_sampler = torch.utils.data.sampler.WeightedRandomSampler(sample_weights, len(train_data), replacement=True)\n",
    "\n",
    "train_loader = prepare_dataloader(\n",
    "    train_data_pt,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    intra_domain_shuffle=True,\n",
    "    drop_last=False,\n",
    "    sampler=train_sampler\n",
    ")\n",
    "valid_loader = prepare_dataloader(\n",
    "    valid_data_pt,\n",
    "    batch_size=eval_batch_size,\n",
    "    shuffle=False,\n",
    "    intra_domain_shuffle=False,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a503c",
   "metadata": {},
   "source": [
    "## Step 4: PEFT scGPT with task-specific objectives\n",
    "Now we are starting PEFT(Parameter-Efficient Fine-Tuning) scGPT. In the cell type identification task, we use CLS objective to train scGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e48b893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - | epoch   1 | 100/835 batches | lr 0.00005 | ms/batch 323.10 | loss  2.39 | cls  2.39 | err  0.71 | \n",
      "scGPT - INFO - | epoch   1 | 200/835 batches | lr 0.00005 | ms/batch 292.97 | loss  1.44 | cls  1.44 | err  0.42 | \n",
      "scGPT - INFO - | epoch   1 | 300/835 batches | lr 0.00005 | ms/batch 293.97 | loss  1.06 | cls  1.06 | err  0.33 | \n",
      "scGPT - INFO - | epoch   1 | 400/835 batches | lr 0.00005 | ms/batch 294.44 | loss  0.88 | cls  0.88 | err  0.29 | \n",
      "scGPT - INFO - | epoch   1 | 500/835 batches | lr 0.00005 | ms/batch 294.36 | loss  0.78 | cls  0.78 | err  0.26 | \n",
      "scGPT - INFO - | epoch   1 | 600/835 batches | lr 0.00005 | ms/batch 294.34 | loss  0.71 | cls  0.71 | err  0.24 | \n",
      "scGPT - INFO - | epoch   1 | 700/835 batches | lr 0.00005 | ms/batch 294.31 | loss  0.64 | cls  0.64 | err  0.23 | \n",
      "scGPT - INFO - | epoch   1 | 800/835 batches | lr 0.00005 | ms/batch 294.35 | loss  0.59 | cls  0.59 | err  0.20 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 256.82s | valid loss/mse 1.1855 | err 0.4775\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 1.1855\n",
      "scGPT - INFO - | epoch   2 | 100/835 batches | lr 0.00005 | ms/batch 299.19 | loss  0.52 | cls  0.52 | err  0.18 | \n",
      "scGPT - INFO - | epoch   2 | 200/835 batches | lr 0.00005 | ms/batch 293.64 | loss  0.47 | cls  0.47 | err  0.16 | \n",
      "scGPT - INFO - | epoch   2 | 300/835 batches | lr 0.00005 | ms/batch 293.72 | loss  0.48 | cls  0.48 | err  0.16 | \n",
      "scGPT - INFO - | epoch   2 | 400/835 batches | lr 0.00005 | ms/batch 293.60 | loss  0.43 | cls  0.43 | err  0.15 | \n",
      "scGPT - INFO - | epoch   2 | 500/835 batches | lr 0.00005 | ms/batch 293.53 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch   2 | 600/835 batches | lr 0.00005 | ms/batch 293.60 | loss  0.38 | cls  0.38 | err  0.13 | \n",
      "scGPT - INFO - | epoch   2 | 700/835 batches | lr 0.00005 | ms/batch 293.55 | loss  0.37 | cls  0.37 | err  0.13 | \n",
      "scGPT - INFO - | epoch   2 | 800/835 batches | lr 0.00005 | ms/batch 293.57 | loss  0.35 | cls  0.35 | err  0.12 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 253.88s | valid loss/mse 0.9034 | err 0.3412\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.9034\n",
      "scGPT - INFO - | epoch   3 | 100/835 batches | lr 0.00004 | ms/batch 299.02 | loss  0.31 | cls  0.31 | err  0.11 | \n",
      "scGPT - INFO - | epoch   3 | 200/835 batches | lr 0.00004 | ms/batch 293.59 | loss  0.30 | cls  0.30 | err  0.10 | \n",
      "scGPT - INFO - | epoch   3 | 300/835 batches | lr 0.00004 | ms/batch 293.69 | loss  0.31 | cls  0.31 | err  0.11 | \n",
      "scGPT - INFO - | epoch   3 | 400/835 batches | lr 0.00004 | ms/batch 293.67 | loss  0.29 | cls  0.29 | err  0.10 | \n",
      "scGPT - INFO - | epoch   3 | 500/835 batches | lr 0.00004 | ms/batch 293.62 | loss  0.29 | cls  0.29 | err  0.10 | \n",
      "scGPT - INFO - | epoch   3 | 600/835 batches | lr 0.00004 | ms/batch 293.45 | loss  0.28 | cls  0.28 | err  0.10 | \n",
      "scGPT - INFO - | epoch   3 | 700/835 batches | lr 0.00004 | ms/batch 293.67 | loss  0.25 | cls  0.25 | err  0.09 | \n",
      "scGPT - INFO - | epoch   3 | 800/835 batches | lr 0.00004 | ms/batch 293.57 | loss  0.25 | cls  0.25 | err  0.09 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 253.87s | valid loss/mse 0.7560 | err 0.2864\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.7560\n",
      "scGPT - INFO - | epoch   4 | 100/835 batches | lr 0.00004 | ms/batch 298.75 | loss  0.25 | cls  0.25 | err  0.08 | \n",
      "scGPT - INFO - | epoch   4 | 200/835 batches | lr 0.00004 | ms/batch 293.33 | loss  0.23 | cls  0.23 | err  0.08 | \n",
      "scGPT - INFO - | epoch   4 | 300/835 batches | lr 0.00004 | ms/batch 293.25 | loss  0.22 | cls  0.22 | err  0.08 | \n",
      "scGPT - INFO - | epoch   4 | 400/835 batches | lr 0.00004 | ms/batch 293.33 | loss  0.22 | cls  0.22 | err  0.07 | \n",
      "scGPT - INFO - | epoch   4 | 500/835 batches | lr 0.00004 | ms/batch 293.23 | loss  0.21 | cls  0.21 | err  0.08 | \n",
      "scGPT - INFO - | epoch   4 | 600/835 batches | lr 0.00004 | ms/batch 293.25 | loss  0.20 | cls  0.20 | err  0.07 | \n",
      "scGPT - INFO - | epoch   4 | 700/835 batches | lr 0.00004 | ms/batch 293.29 | loss  0.20 | cls  0.20 | err  0.07 | \n",
      "scGPT - INFO - | epoch   4 | 800/835 batches | lr 0.00004 | ms/batch 293.12 | loss  0.21 | cls  0.21 | err  0.07 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 253.55s | valid loss/mse 0.6638 | err 0.2297\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.6638\n",
      "scGPT - INFO - | epoch   5 | 100/835 batches | lr 0.00003 | ms/batch 298.61 | loss  0.17 | cls  0.17 | err  0.06 | \n",
      "scGPT - INFO - | epoch   5 | 200/835 batches | lr 0.00003 | ms/batch 293.41 | loss  0.18 | cls  0.18 | err  0.06 | \n",
      "scGPT - INFO - | epoch   5 | 300/835 batches | lr 0.00003 | ms/batch 293.37 | loss  0.18 | cls  0.18 | err  0.06 | \n",
      "scGPT - INFO - | epoch   5 | 400/835 batches | lr 0.00003 | ms/batch 293.33 | loss  0.17 | cls  0.17 | err  0.06 | \n",
      "scGPT - INFO - | epoch   5 | 500/835 batches | lr 0.00003 | ms/batch 293.42 | loss  0.17 | cls  0.17 | err  0.06 | \n",
      "scGPT - INFO - | epoch   5 | 600/835 batches | lr 0.00003 | ms/batch 293.25 | loss  0.16 | cls  0.16 | err  0.05 | \n",
      "scGPT - INFO - | epoch   5 | 700/835 batches | lr 0.00003 | ms/batch 293.29 | loss  0.16 | cls  0.16 | err  0.06 | \n",
      "scGPT - INFO - | epoch   5 | 800/835 batches | lr 0.00003 | ms/batch 293.32 | loss  0.15 | cls  0.15 | err  0.05 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 253.60s | valid loss/mse 0.6383 | err 0.2087\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.6383\n",
      "scGPT - INFO - | epoch   6 | 100/835 batches | lr 0.00003 | ms/batch 298.78 | loss  0.16 | cls  0.16 | err  0.06 | \n",
      "scGPT - INFO - | epoch   6 | 200/835 batches | lr 0.00003 | ms/batch 293.24 | loss  0.15 | cls  0.15 | err  0.05 | \n",
      "scGPT - INFO - | epoch   6 | 300/835 batches | lr 0.00003 | ms/batch 293.30 | loss  0.15 | cls  0.15 | err  0.05 | \n",
      "scGPT - INFO - | epoch   6 | 400/835 batches | lr 0.00003 | ms/batch 293.21 | loss  0.14 | cls  0.14 | err  0.05 | \n",
      "scGPT - INFO - | epoch   6 | 500/835 batches | lr 0.00003 | ms/batch 293.15 | loss  0.14 | cls  0.14 | err  0.05 | \n",
      "scGPT - INFO - | epoch   6 | 600/835 batches | lr 0.00003 | ms/batch 293.28 | loss  0.13 | cls  0.13 | err  0.04 | \n",
      "scGPT - INFO - | epoch   6 | 700/835 batches | lr 0.00003 | ms/batch 293.18 | loss  0.14 | cls  0.14 | err  0.05 | \n",
      "scGPT - INFO - | epoch   6 | 800/835 batches | lr 0.00003 | ms/batch 293.40 | loss  0.13 | cls  0.13 | err  0.05 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 253.58s | valid loss/mse 0.6065 | err 0.2054\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.6065\n",
      "scGPT - INFO - | epoch   7 | 100/835 batches | lr 0.00003 | ms/batch 298.78 | loss  0.13 | cls  0.13 | err  0.05 | \n",
      "scGPT - INFO - | epoch   7 | 200/835 batches | lr 0.00003 | ms/batch 293.36 | loss  0.12 | cls  0.12 | err  0.04 | \n",
      "scGPT - INFO - | epoch   7 | 300/835 batches | lr 0.00003 | ms/batch 293.40 | loss  0.13 | cls  0.13 | err  0.05 | \n",
      "scGPT - INFO - | epoch   7 | 400/835 batches | lr 0.00003 | ms/batch 293.28 | loss  0.11 | cls  0.11 | err  0.04 | \n",
      "scGPT - INFO - | epoch   7 | 500/835 batches | lr 0.00003 | ms/batch 293.50 | loss  0.12 | cls  0.12 | err  0.04 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - | epoch   7 | 600/835 batches | lr 0.00003 | ms/batch 293.44 | loss  0.12 | cls  0.12 | err  0.04 | \n",
      "scGPT - INFO - | epoch   7 | 700/835 batches | lr 0.00003 | ms/batch 293.43 | loss  0.12 | cls  0.12 | err  0.04 | \n",
      "scGPT - INFO - | epoch   7 | 800/835 batches | lr 0.00003 | ms/batch 293.26 | loss  0.13 | cls  0.13 | err  0.04 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 253.64s | valid loss/mse 0.5606 | err 0.1679\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.5606\n",
      "scGPT - INFO - | epoch   8 | 100/835 batches | lr 0.00002 | ms/batch 298.49 | loss  0.11 | cls  0.11 | err  0.04 | \n",
      "scGPT - INFO - | epoch   8 | 200/835 batches | lr 0.00002 | ms/batch 293.26 | loss  0.10 | cls  0.10 | err  0.04 | \n",
      "scGPT - INFO - | epoch   8 | 300/835 batches | lr 0.00002 | ms/batch 293.20 | loss  0.12 | cls  0.12 | err  0.04 | \n",
      "scGPT - INFO - | epoch   8 | 400/835 batches | lr 0.00002 | ms/batch 293.16 | loss  0.10 | cls  0.10 | err  0.04 | \n",
      "scGPT - INFO - | epoch   8 | 500/835 batches | lr 0.00002 | ms/batch 293.26 | loss  0.12 | cls  0.12 | err  0.04 | \n",
      "scGPT - INFO - | epoch   8 | 600/835 batches | lr 0.00002 | ms/batch 293.23 | loss  0.10 | cls  0.10 | err  0.04 | \n",
      "scGPT - INFO - | epoch   8 | 700/835 batches | lr 0.00002 | ms/batch 293.24 | loss  0.11 | cls  0.11 | err  0.04 | \n",
      "scGPT - INFO - | epoch   8 | 800/835 batches | lr 0.00002 | ms/batch 293.27 | loss  0.11 | cls  0.11 | err  0.04 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 253.55s | valid loss/mse 0.5441 | err 0.1530\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.5441\n",
      "scGPT - INFO - | epoch   9 | 100/835 batches | lr 0.00002 | ms/batch 298.85 | loss  0.10 | cls  0.10 | err  0.04 | \n",
      "scGPT - INFO - | epoch   9 | 200/835 batches | lr 0.00002 | ms/batch 293.33 | loss  0.09 | cls  0.09 | err  0.03 | \n",
      "scGPT - INFO - | epoch   9 | 300/835 batches | lr 0.00002 | ms/batch 293.34 | loss  0.10 | cls  0.10 | err  0.04 | \n",
      "scGPT - INFO - | epoch   9 | 400/835 batches | lr 0.00002 | ms/batch 293.37 | loss  0.10 | cls  0.10 | err  0.04 | \n",
      "scGPT - INFO - | epoch   9 | 500/835 batches | lr 0.00002 | ms/batch 293.38 | loss  0.10 | cls  0.10 | err  0.04 | \n",
      "scGPT - INFO - | epoch   9 | 600/835 batches | lr 0.00002 | ms/batch 293.44 | loss  0.10 | cls  0.10 | err  0.03 | \n",
      "scGPT - INFO - | epoch   9 | 700/835 batches | lr 0.00002 | ms/batch 293.44 | loss  0.09 | cls  0.09 | err  0.03 | \n",
      "scGPT - INFO - | epoch   9 | 800/835 batches | lr 0.00002 | ms/batch 293.49 | loss  0.09 | cls  0.09 | err  0.03 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   9 | time: 253.69s | valid loss/mse 0.5660 | err 0.1646\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch  10 | 100/835 batches | lr 0.00002 | ms/batch 298.78 | loss  0.10 | cls  0.10 | err  0.04 | \n",
      "scGPT - INFO - | epoch  10 | 200/835 batches | lr 0.00002 | ms/batch 293.34 | loss  0.09 | cls  0.09 | err  0.04 | \n",
      "scGPT - INFO - | epoch  10 | 300/835 batches | lr 0.00002 | ms/batch 293.41 | loss  0.09 | cls  0.09 | err  0.03 | \n",
      "scGPT - INFO - | epoch  10 | 400/835 batches | lr 0.00002 | ms/batch 293.36 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - | epoch  10 | 500/835 batches | lr 0.00002 | ms/batch 293.48 | loss  0.10 | cls  0.10 | err  0.03 | \n",
      "scGPT - INFO - | epoch  10 | 600/835 batches | lr 0.00002 | ms/batch 293.48 | loss  0.09 | cls  0.09 | err  0.03 | \n",
      "scGPT - INFO - | epoch  10 | 700/835 batches | lr 0.00002 | ms/batch 293.49 | loss  0.09 | cls  0.09 | err  0.03 | \n",
      "scGPT - INFO - | epoch  10 | 800/835 batches | lr 0.00002 | ms/batch 293.61 | loss  0.09 | cls  0.09 | err  0.03 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  10 | time: 253.68s | valid loss/mse 0.5368 | err 0.1548\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.5368\n",
      "scGPT - INFO - | epoch  11 | 100/835 batches | lr 0.00002 | ms/batch 298.99 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - | epoch  11 | 200/835 batches | lr 0.00002 | ms/batch 293.41 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - | epoch  11 | 300/835 batches | lr 0.00002 | ms/batch 293.18 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - | epoch  11 | 400/835 batches | lr 0.00002 | ms/batch 293.50 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - | epoch  11 | 500/835 batches | lr 0.00002 | ms/batch 293.27 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - | epoch  11 | 600/835 batches | lr 0.00002 | ms/batch 293.30 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - | epoch  11 | 700/835 batches | lr 0.00002 | ms/batch 293.26 | loss  0.07 | cls  0.07 | err  0.03 | \n",
      "scGPT - INFO - | epoch  11 | 800/835 batches | lr 0.00002 | ms/batch 293.31 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  11 | time: 253.68s | valid loss/mse 0.5441 | err 0.1485\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch  12 | 100/835 batches | lr 0.00002 | ms/batch 298.64 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - | epoch  12 | 200/835 batches | lr 0.00002 | ms/batch 293.35 | loss  0.07 | cls  0.07 | err  0.03 | \n",
      "scGPT - INFO - | epoch  12 | 300/835 batches | lr 0.00002 | ms/batch 293.43 | loss  0.07 | cls  0.07 | err  0.03 | \n",
      "scGPT - INFO - | epoch  12 | 400/835 batches | lr 0.00002 | ms/batch 293.39 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - | epoch  12 | 500/835 batches | lr 0.00002 | ms/batch 293.42 | loss  0.09 | cls  0.09 | err  0.03 | \n",
      "scGPT - INFO - | epoch  12 | 600/835 batches | lr 0.00002 | ms/batch 293.50 | loss  0.07 | cls  0.07 | err  0.03 | \n",
      "scGPT - INFO - | epoch  12 | 700/835 batches | lr 0.00002 | ms/batch 293.45 | loss  0.09 | cls  0.09 | err  0.03 | \n",
      "scGPT - INFO - | epoch  12 | 800/835 batches | lr 0.00002 | ms/batch 293.37 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  12 | time: 253.70s | valid loss/mse 0.5239 | err 0.1418\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.5239\n",
      "scGPT - INFO - | epoch  13 | 100/835 batches | lr 0.00001 | ms/batch 298.93 | loss  0.07 | cls  0.07 | err  0.03 | \n",
      "scGPT - INFO - | epoch  13 | 200/835 batches | lr 0.00001 | ms/batch 293.51 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - | epoch  13 | 300/835 batches | lr 0.00001 | ms/batch 293.41 | loss  0.07 | cls  0.07 | err  0.02 | \n",
      "scGPT - INFO - | epoch  13 | 400/835 batches | lr 0.00001 | ms/batch 293.29 | loss  0.07 | cls  0.07 | err  0.03 | \n",
      "scGPT - INFO - | epoch  13 | 500/835 batches | lr 0.00001 | ms/batch 293.45 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - | epoch  13 | 600/835 batches | lr 0.00001 | ms/batch 293.25 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - | epoch  13 | 700/835 batches | lr 0.00001 | ms/batch 293.43 | loss  0.07 | cls  0.07 | err  0.03 | \n",
      "scGPT - INFO - | epoch  13 | 800/835 batches | lr 0.00001 | ms/batch 293.33 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  13 | time: 253.65s | valid loss/mse 0.5205 | err 0.1438\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.5205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - | epoch  14 | 100/835 batches | lr 0.00001 | ms/batch 298.52 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - | epoch  14 | 200/835 batches | lr 0.00001 | ms/batch 293.38 | loss  0.07 | cls  0.07 | err  0.02 | \n",
      "scGPT - INFO - | epoch  14 | 300/835 batches | lr 0.00001 | ms/batch 293.33 | loss  0.07 | cls  0.07 | err  0.02 | \n",
      "scGPT - INFO - | epoch  14 | 400/835 batches | lr 0.00001 | ms/batch 293.34 | loss  0.07 | cls  0.07 | err  0.02 | \n",
      "scGPT - INFO - | epoch  14 | 500/835 batches | lr 0.00001 | ms/batch 293.43 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  14 | 600/835 batches | lr 0.00001 | ms/batch 293.54 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  14 | 700/835 batches | lr 0.00001 | ms/batch 293.46 | loss  0.07 | cls  0.07 | err  0.03 | \n",
      "scGPT - INFO - | epoch  14 | 800/835 batches | lr 0.00001 | ms/batch 293.35 | loss  0.07 | cls  0.07 | err  0.03 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  14 | time: 253.65s | valid loss/mse 0.5227 | err 0.1499\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch  15 | 100/835 batches | lr 0.00001 | ms/batch 298.81 | loss  0.07 | cls  0.07 | err  0.02 | \n",
      "scGPT - INFO - | epoch  15 | 200/835 batches | lr 0.00001 | ms/batch 293.39 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  15 | 300/835 batches | lr 0.00001 | ms/batch 293.30 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  15 | 400/835 batches | lr 0.00001 | ms/batch 293.35 | loss  0.07 | cls  0.07 | err  0.03 | \n",
      "scGPT - INFO - | epoch  15 | 500/835 batches | lr 0.00001 | ms/batch 293.41 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  15 | 600/835 batches | lr 0.00001 | ms/batch 293.36 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  15 | 700/835 batches | lr 0.00001 | ms/batch 293.34 | loss  0.07 | cls  0.07 | err  0.02 | \n",
      "scGPT - INFO - | epoch  15 | 800/835 batches | lr 0.00001 | ms/batch 293.34 | loss  0.07 | cls  0.07 | err  0.02 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  15 | time: 253.62s | valid loss/mse 0.4979 | err 0.1336\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4979\n",
      "scGPT - INFO - | epoch  16 | 100/835 batches | lr 0.00001 | ms/batch 298.73 | loss  0.07 | cls  0.07 | err  0.03 | \n",
      "scGPT - INFO - | epoch  16 | 200/835 batches | lr 0.00001 | ms/batch 293.42 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  16 | 300/835 batches | lr 0.00001 | ms/batch 293.25 | loss  0.07 | cls  0.07 | err  0.02 | \n",
      "scGPT - INFO - | epoch  16 | 400/835 batches | lr 0.00001 | ms/batch 293.17 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  16 | 500/835 batches | lr 0.00001 | ms/batch 293.45 | loss  0.07 | cls  0.07 | err  0.02 | \n",
      "scGPT - INFO - | epoch  16 | 600/835 batches | lr 0.00001 | ms/batch 293.31 | loss  0.07 | cls  0.07 | err  0.02 | \n",
      "scGPT - INFO - | epoch  16 | 700/835 batches | lr 0.00001 | ms/batch 293.39 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  16 | 800/835 batches | lr 0.00001 | ms/batch 293.37 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  16 | time: 253.60s | valid loss/mse 0.5033 | err 0.1329\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch  17 | 100/835 batches | lr 0.00001 | ms/batch 298.58 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  17 | 200/835 batches | lr 0.00001 | ms/batch 293.33 | loss  0.07 | cls  0.07 | err  0.02 | \n",
      "scGPT - INFO - | epoch  17 | 300/835 batches | lr 0.00001 | ms/batch 293.42 | loss  0.07 | cls  0.07 | err  0.02 | \n",
      "scGPT - INFO - | epoch  17 | 400/835 batches | lr 0.00001 | ms/batch 293.50 | loss  0.07 | cls  0.07 | err  0.02 | \n",
      "scGPT - INFO - | epoch  17 | 500/835 batches | lr 0.00001 | ms/batch 293.34 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  17 | 600/835 batches | lr 0.00001 | ms/batch 293.40 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  17 | 700/835 batches | lr 0.00001 | ms/batch 293.50 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  17 | 800/835 batches | lr 0.00001 | ms/batch 293.30 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  17 | time: 253.63s | valid loss/mse 0.5015 | err 0.1300\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch  18 | 100/835 batches | lr 0.00001 | ms/batch 299.00 | loss  0.05 | cls  0.05 | err  0.02 | \n",
      "scGPT - INFO - | epoch  18 | 200/835 batches | lr 0.00001 | ms/batch 293.23 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  18 | 300/835 batches | lr 0.00001 | ms/batch 293.29 | loss  0.05 | cls  0.05 | err  0.02 | \n",
      "scGPT - INFO - | epoch  18 | 400/835 batches | lr 0.00001 | ms/batch 293.28 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  18 | 500/835 batches | lr 0.00001 | ms/batch 293.39 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  18 | 600/835 batches | lr 0.00001 | ms/batch 293.38 | loss  0.06 | cls  0.06 | err  0.03 | \n",
      "scGPT - INFO - | epoch  18 | 700/835 batches | lr 0.00001 | ms/batch 293.32 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  18 | 800/835 batches | lr 0.00001 | ms/batch 293.35 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  18 | time: 253.62s | valid loss/mse 0.4911 | err 0.1276\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4911\n",
      "scGPT - INFO - | epoch  19 | 100/835 batches | lr 0.00001 | ms/batch 298.89 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  19 | 200/835 batches | lr 0.00001 | ms/batch 293.36 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  19 | 300/835 batches | lr 0.00001 | ms/batch 293.21 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  19 | 400/835 batches | lr 0.00001 | ms/batch 293.37 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  19 | 500/835 batches | lr 0.00001 | ms/batch 293.24 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  19 | 600/835 batches | lr 0.00001 | ms/batch 293.03 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  19 | 700/835 batches | lr 0.00001 | ms/batch 293.22 | loss  0.05 | cls  0.05 | err  0.02 | \n",
      "scGPT - INFO - | epoch  19 | 800/835 batches | lr 0.00001 | ms/batch 293.30 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  19 | time: 253.56s | valid loss/mse 0.4962 | err 0.1224\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch  20 | 100/835 batches | lr 0.00001 | ms/batch 298.50 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  20 | 200/835 batches | lr 0.00001 | ms/batch 293.33 | loss  0.05 | cls  0.05 | err  0.02 | \n",
      "scGPT - INFO - | epoch  20 | 300/835 batches | lr 0.00001 | ms/batch 293.23 | loss  0.05 | cls  0.05 | err  0.02 | \n",
      "scGPT - INFO - | epoch  20 | 400/835 batches | lr 0.00001 | ms/batch 293.25 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  20 | 500/835 batches | lr 0.00001 | ms/batch 293.25 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  20 | 600/835 batches | lr 0.00001 | ms/batch 293.19 | loss  0.06 | cls  0.06 | err  0.02 | \n",
      "scGPT - INFO - | epoch  20 | 700/835 batches | lr 0.00001 | ms/batch 293.23 | loss  0.06 | cls  0.06 | err  0.02 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - | epoch  20 | 800/835 batches | lr 0.00001 | ms/batch 293.33 | loss  0.05 | cls  0.05 | err  0.02 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  20 | time: 253.56s | valid loss/mse 0.5027 | err 0.1269\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    if config.do_train:\n",
    "        train(\n",
    "            model,\n",
    "            loader=train_loader,\n",
    "        )\n",
    "    val_loss, val_err = evaluate(\n",
    "        model,\n",
    "        loader=valid_loader,\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} | err {val_err:5.4f}\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= early_stop:\n",
    "            logger.info(f\"Early stop at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    if DAB_separate_optim:\n",
    "        scheduler_dab.step()\n",
    "    if ADV:\n",
    "        scheduler_D.step()\n",
    "        scheduler_E.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a6ce176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% inference\n",
    "def test(model: nn.Module, adata: DataLoader) -> float:\n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = adata.obs[\"celltype_id\"].tolist()  # make sure count from 0\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    tokenized_test = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=include_zero_gene,\n",
    "    )\n",
    "\n",
    "    input_values_test = random_mask_value(\n",
    "        tokenized_test[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "\n",
    "    test_data_pt = {\n",
    "        \"gene_ids\": tokenized_test[\"genes\"],\n",
    "        \"values\": input_values_test,\n",
    "        \"target_values\": tokenized_test[\"values\"],\n",
    "        \"batch_labels\": torch.from_numpy(batch_ids).long(),\n",
    "        \"celltype_labels\": torch.from_numpy(celltypes_labels).long(),\n",
    "    }\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=SeqDataset(test_data_pt),\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=min(len(os.sched_getaffinity(0)), eval_batch_size // 2),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    predictions = evaluate(\n",
    "        model,\n",
    "        loader=test_loader,\n",
    "        return_raw=True,\n",
    "    )\n",
    "\n",
    "    # compute accuracy, precision, recall, f1\n",
    "    from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    accuracy = balanced_accuracy_score(celltypes_labels, predictions)\n",
    "    precision = precision_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    recall = recall_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    macro_f1 = f1_score(celltypes_labels, predictions, average=\"macro\")\n",
    "\n",
    "    logger.info(\n",
    "        f\"Balanced accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, \"\n",
    "        f\"Macro F1: {macro_f1:.3f}\"\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"test/accuracy\": accuracy,\n",
    "        \"test/precision\": precision,\n",
    "        \"test/recall\": recall,\n",
    "        \"test/macro_f1\": macro_f1,\n",
    "    }\n",
    "\n",
    "    return predictions, celltypes_labels, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16236bf2",
   "metadata": {},
   "source": [
    "## Step 5: Inference with PEFT scGPT model\n",
    "In the cross species task, the PEFT scGPT predicts cell-type labels for query set as inference. The model performance is evaluated on standard classificaton metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79730e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Balanced accuracy: 0.760, Precision: 0.549, Recall: 0.626, Macro F1: 0.550\n"
     ]
    }
   ],
   "source": [
    "predictions, labels, results = test(best_model, adata_test)\n",
    "\n",
    "save_dict = {\n",
    "    \"predictions\": predictions,\n",
    "    \"labels\": labels,\n",
    "    \"results\": results,\n",
    "    \"id_maps\": id2type\n",
    "}\n",
    "with open(save_dir / \"results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(save_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62ebbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model into the save_dir\n",
    "torch.save(best_model.state_dict(), save_dir / \"model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scGPT",
   "language": "python",
   "name": "scgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
